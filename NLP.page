TODO merge in notes from 6.864
shingles
markov chain
pos tagging
logistic
stemming
  porter stemming: 
  exclusion lists

general NLP problems

- POS tagging
- sentence detection
- tokenization
- chunking
- parsing
- named entity recognition
- coreference

misc

- $n$-grams of chars: useful for language guessing

term frequency inverse document frequency (TFIDF)

- term frequency is num occurrences of term $t_i$ in doc $d_j$ over total num
  words in $d_j$ (how prominent $t_i$ is in $d_j$):

  $$
  tf_{i,j} = \frac{ n_{i,j} }{ \sum_k n_{k,j} }
  $$

- inverse document frequency is num docs over num docs with $t_i$ (more common
  words are less important):

  $$
  idf_i = \log \frac{ |D| }{ | \{ d : t_i \in d \} | }
  $$

- tf-idf = tf * idf

soundex algorithm

- rudemintary "sounds-alike" phonetic algorithm for encoding things that sound
  similar to the same representation, for matching

vector space model

- represent text docs as vectors of identifiers
- eg index terms: each dim corresponds to a term; some possible values are:
  - bits indicating presence
  - counts
  - tf-idf
- relevancy rankings: treat query as doc, find most cosine-similar doc
- limitations
  - long docs poorly represented, have poor similarity values (small scalar
    product/numerator and large dimensionality/denominator)
  - order information not preserved

cosine similarity

- intuitively, find angle between vector representations of docs
- don't actually need to calc cosine; can just leave as is; ordering preserved
- $-1$ means opposite, 0 means same, 1 means identical; in IR, components are
  always non-neg, so range is [0,1]

jaccard similarity/coefficient/index

- simple: $J(A,B) = \frac{|A \cap B|}{|A \cup B|}$
