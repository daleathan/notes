- graph theory: a branch of combinatorics
  - eulerian circuit iff all nodes have even degrees
  - eulerian non-circuit tour iff only 2 nodes have odd degrees

statistical learning
====================

- kernel density estimator: basically place a small kernel everywhere you have
  a sample, resulting in somtehing much smoother than a histogram (where bins
  can make all the difference in interpretation)

statistics
==========

- areas: descriptive, inferential

- moments
  - positive skew: lumped to the left (counter-intuitive)
  - positive kurtosis: pointier peak

- sample variance: two definitions, one divides by $n-1$ instead of $n$

- mean deviation: the mean absolute deviation from the mean
  - harder to eval in closed form
  - used by TPC To calculate large deviations from RTT by calcing EWM deviation

- central limit theorem: mean/sum of sufficiently many independent random
  variables, each with finite mean & variance, is approximately normal
  - as sample size increases, sample mean distribution approaches normal

- TODO
- (paired) student t-test
- distributions
  - student t-distribution: something about normals for small populations where
    variance is unknown
  - chi-square distribution
- $\mathrm{Correlation}(X, Y) = \frac{ \mathrm{Covariance}(X, Y) }{ \sigma_X
  \sigma_Y }$ where $\sigma$ is the SD

- Kendall tau rank correlation coefficient $\tau$
  - similarity of orderings of data
  - TODO uses p-value somewhere

test accuracy

- precision = relevant & retrieved / retrieved
- recall = relevant & retrieved / relevant
- F1 score aka F-measure: `F = 2 * (precision * recall) / (precision + recall)`

statistical significance

- standard score: number of SDs; aka z-value, z-score, normal score
- p-value: significance level; P(observation|null hypothesis)
  - prob of mistakenly rejecting null hyp < p-value
  - the lower, the less likely the result if the null hyp is true, thus the
    more statistically significant the result is
  - if < .05 or .01, corresponding to a 5% or 1% chance of rejecting the null
    hyp when it's actually true (type I error)
  - 5% is a typical value
  - eg P(14+ heads of 20 flips|fair coin)
- E-value: expected num tests until a certain observation, assuming null hyp
- how to efficiently calc prob of *at least* $h$ H in $n$ flips?
- type I error: false pos; rej null hyp when it's actually true
- type II error: false neg; acc null hyp when it's actually false
- don't stop experiment as soon as significant results are detected
  - <http://www.evanmiller.org/how-not-to-run-an-ab-test.html>

- TODO ridge regression

- mean absolute percentage error (MAPE): $M = \frac{1}{n} \bigsum_{t=1}^n
  \left\vert \frac{A_t - F_t}{A_t} \right\vert$

combinatorics
=============

- permutations without replacement: P(n,r) = n! / (n-r)!
- permutations with replacement: n^r
- combinations without replacement (partitioning sets): C(n,k) = n! / (k! (n-k)!)
  - this is the _binomial coefficient_
  - _multinomial coefficient_: partitioning into multiple sets
    - binomial is actually C(n; k, n-k)
    - C(n; k1, k2, ..., kp) = n! / (k1! k2! ... kp!)
- combinations with replacement (multisets/bags):
  C(n-1 + k; n-1, k) = (n-1+k)! / (k! (n-1)!)
  - proof using "stars and bars"

mathematics
===========

- abstract algebra: study of algebraic structures such as groups, rings,
  fields, modules, vector spaces, and algebras
- topology: extension of geometry that focuses on structure
- algebraic topology: uses abstract algebra
- graph theory
- topological graph theory: topology + graph theory; studies embeddings of
  graphs in surfaces
- number theory
- computational number theory, algorithmic number theory: applications in
  crypto

\begin{equation}
e^x = 1 + x \left(
      1 + x \left(
      \frac{1}{2} + x \left(
      \frac{1}{6} + x \left(
      \frac{1}{24} + x \left( \dots \right) \right) \right) \right) \right)
\end{equation}

geometry
========

- Pythagorean theorem
  - easy proof

Interesting questions that came up during ICFPC08

- find the tangent lines (up to four) between a pair of circles
  - find the tangent lines between a circle and a point
- if we roll a circle around the outside of an ellipse and trace the path of
  its center, is the resulting oval an ellipse?

linear algebra
==============

From <http://en.wikipedia.org/wiki/Sparse_matrix>:

> Conceptually, sparsity corresponds to systems which are loosely coupled.
> Consider a line of balls connected by springs from one to the next; this is a
> sparse system. By contrast, if the same line of balls had springs connecting
> every ball to every other ball, the system would be represented by a dense
> matrix. The concept of sparsity is useful in combinatorics and application
> areas such as network theory, of a low density of significant data or
> connections.

linear independence

- vectors linearly dependent iff determinant of vectors as cols is 0

symmetric positive definite, positive semi-definite, negative definite, indefinite, etc.

- $M$ is _positive definite_ iff any of:
  - $z^T M z > 0$ for all nonzero vector $z$
  - all evals of $M$ are pos
  - all leading principle minors are pos
    - i.e. upper left $k \times k$ submatrix of $M$ has pos det for all $k =
      1,2,\dots,n$
    - eg: if $M$ is diagonal, then $M$ is pos def iff all diag elts are pos
      (diag elts are evals)
- $M$ is neg def iff any of:
  - $-M$ is pos def, i.e. $x^T M x < 0$ for all nonzero vector $x$
  - all evals of $M$ are neg
  - upper left $1 \times 1$ submatrix has pos det, upper left $2 \times 2$
    submatrix has neg det, upper left $3 \times 3$ submatrix has pos det, etc.
- $M$ is pos semi-def iff any of:
  - $z^T M z \ge 0$ for all vector $z$ (usual definition)
  - all evals of $M$ are non-neg
  - note: if $M$ is pos semi-def, then all upper left $k \times k$ submatrices
    have non-neg dets, but this is not a sufficient condition any more; same
    for neg semi-def below
- $M$ is neg semi-def iff any of:
  - $z^T M z \le 0$ for all vector $z$ (usual definition)
  - all evals of $M$ are non-pos
  $M$ is indef iff any of:
    - $M$ is neither pos semi-def nor neg semi-def
    - $M$ has pos and neg evals

misc

- A matrix $A$ is diagonalizable if there exists a non-singular matrix $B$ and
  a diagonal matrix $D$ such that $A = BDB^{-1}$.
- _singular value decomposition (SVD)_: important factorization of a
  rectangular real or complex matrix
  - many applications in signal processing, statistics
  - eg computing the pseudoinverse, least squares fitting of data, matrix
    approximation, and determining the rank, range and null space of a matrix
- a matrix is invertible iff all its eigenvalues are non-zero
- if $M$ is pos def then $z^* M z > 0$ for any vector z
- a pos def matrix has pos def inverse
- all eigenvalues of pos def are positive
- a real sym matrix has real sym inverse
- _kernel_ of a mapping: elts that map to the zero elt
- _kernel_ aka _null space_ of a matrix: nullity of A

foundations

- gaussian elimination: use _elementary row operations_ to get _row
  echelon form_
  - ie subtract rows from each other till lower zero triangle
- invertible = non-singular
- matrix mult: associative, non-commutative
- gauss-jordan elimination: extension of gaussian elimination to get _reduced
  row echelon form_
  - useful for finding inverse
  - $[AI] \Rightarrow [IA^{-1}]$
- LU factorization aka LU decomposition: factorize matrix into product of
  _lower-triangular_ and _upper-triangular_ matrices
  - $A^{-1} = U^{-1} L^{-1}$
  - find using extension of gaussian elim
- $(AB)^{-1} = B^{-1} A^{-1}$; $(A^\mathrm{T})^{-1} = (A^{-1})^\mathrm{T}$
- _symmetric_: transpose is self; inverse is also symmetric
- _vector space_: eg $\mathbf{R}^2$ (set of all 2-dim vecs)
  - _subspace_: subset of vector space closed under linear combination
    - subspaces of $\mathbf{R}^2$: $\mathbf{R}^2$, any line through origin, and
      $\{(0,0)\}$
    - subspaces of $\mathbf{R}^3$: $\mathbf{R}^3$, any plane through origin,
      any line through origin, and $\{(0,0)\}$
  - _column space_ $C(A)$ is set of all linear combinations of $A$'s columns
    - if not fully linearly indep, then fewer dimensions spanned then #columns
    - $Ax=b$ can be solved iff $b \in C(A)$
  - _null space_ $N(A)$ is set of all solutions to $Ax=0$
    - solutions $x$ always form a subspace, i.e. linear combos of $x$s are 0

- TODO
  - characteristic polynomial: $\det(A - lambda I) = 0$
  - stochastic matrix
  - eigen*, singular, etc.
  - http://www.cut-the-knot.org/arithmetic/algebra/eigens.shtml
  - spectral decomposition aka eigendecomposition
  - spectral theorem

cryptography
============

RSA (TODO REVIEW!)

- choose primes p, q
  - primality testing: _wheel factorization_ is certain; there are also
    probabilistic tests
- find $e$ such that $gcd(e, (p-1)(q-1)) = 1$
  - i.e., find $a$ such that $ae + b(p-1)(q-1) = 1$. This is a _diophantine
    equation_ and can be solved using the _Euclidian algorithm_.
- exponentiation: find $M^e \mod n$
  - $M$ is data, $e$ is pub key, $n$ is priv key
  - _square and multiply algorithm_
  - _Chinese Remainder Theorem (CRT)_ for more complex, faster algos

How To Share A Secret (Shamir 79)

- split into $n$ pieces such that any $k$ can reconstruct but $k-1$ can't
- <http://research.swtch.com/2008/01/i-could-tell-you-but.html>

statistics
==========

- correlation
  - Pearson's product-moment coefficient: most popular

    $$\rho_{X,Y} = \frac{ \cov(X,Y) }{ \sigma_X \sigma_Y }$$

    - always in $[-1, 1]$
- T-test: TODO

outlier elimination

- chauvenet's criterion
- pierce's criterion: "if you need rigor, generality, and the ability to reject more than one data point"
  - process involves multiple steps and lookup tables
  - <https://web.chemistry.ohio-state.edu/~coe/research/documents/error_distributions_new2.pdf>

exponential smoothing for time series analysis

- _exponential moving average (EMA)_ aka _exponentially weighted moving average
  (EWMA)_ aka _single exponential smoothing (SES)_: does _not_ spot trends
- _double exponential moving average (DEMA)_ aka _linear exponential smoothing
  (LES)_: incorporates seasonality
- _triple exponential moving average (TEMA)_ aka _triple exponential smoothing
  (TES)_: incorporates seasonality & trend
- <http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc43.htm>

signals, coding, information theory
===================================

Audio & Video Compression (Keith Winstein's SIPB cluedump 2008/11/06)

- huffman code: recurse: least freq are same len, differ only in last bit
- keithw: slides
- entropy coding
- arithmetic coding can get us flatter?

general

- Shannon's theorem: describes _channel capacity_, the max information rate at
  which reliable comm is possible over a channel that has a certain error prob
  or SNR
  - only existential, not constructive, hence actual capacity depends on algo

maximum entropy

- given constraints ("testable information"), the dist that best represents the
  current state of knowledge is the one w largest entropy
  - i.e., choose the _least informative_ (and thus most generalizable) dist
    given what we know, since we don't know anything else
  - also, many physical systems tend to move toward maxent configs over time
  - makes explicit our freedom in using different forms of prior information
- applications
  - obtain priors for bayesian inference
  - maxent models: observed data itself is the testable information
- examples
  - given just mean and sd, use normal
  - given just the support [a,b], use uniform over [a,b]
  - exponential dist w mean $1/\lambda$ is maxent dist over all dists in
    $[0,\infty]$ w mean $1/\lambda$

shannon entropy

- $H(X) = E(I(X)) = \sum_i p(x_i) I(x_i) = - \sum_i p(x_i) \log p(x_i)$,
  assuming $X$ has possible values $x_i$
  - $I(X)$ is the _information content_ of $X$
  - if $p_i = 0$, the $0 \log 0$ is taken to be 0
- eg binary entropy: [$H(X)$ for varying
  $P(X=1)$](http://en.wikipedia.org/wiki/File:Binary_entropy_plot.svg)
- distinguish btwn _entropy_ of a distribution of messages/symbols and the
  _entropy rate_ of a stochastic process
  - eg need 7 bits to distinguish the first 128 symbols generated in the
    fibonacci sequence
  - the fib sequence formula itself has a much lower entropy and applies to any
    length fib seq

probability
===========

- poisson process: stream of continuous & independent events occurring at const
  avg rate
  - characterized by rate param $\lambda$
- bernoulli process: stream of discrete & independent events, usu binary

distributions

- bernoulli: heads/tails
- poisson: # events over fixed time in poisson process
  - mean = variance
- exponential: time btwn events in poisson process
- gamma: sum of fixed # exponentially dist vars
  - eg total time for some # ppl to arrive

misc

- $P(X < min(Y,Z)) = P(X > max(Y, Z)) + 1 – P(X > Y) – P(X > Z)$
- Chebyshev's inequality: $P(|X-\mu| \ge k \sigma) \le \frac{1}{k^2}$ for any real $k>0$
- Jensen's inequality: If $f$ is convex and $X$ is a random variable, $f(E(X))
  \le E(f(X))$.

problems

- expected # flips before HTH > that of HTT, because HTH overlaps with itself
  - sketch: with a million flips, same # HTH and HTT, but HTH are more clumped,
    so more space btwn them
- st. petersburg's paradox <http://mindyourdecisions.com/blog/2010/06/07/the-st-petersburg-paradox-a-flimsy-critique-of-expectation-theory-by-people-who-dont-know-math-or-economics/>
  - game: toss coin until H; payoff is $2 for 0T, $4 for 1T, $8 for 2T, ...
  - expected payout is $1/2 * 2 + 1/4 * 4 + 1/8 * 8 + ... = \infty$
  - but must consider realistic payouts (nobody has $\infty$), utility of
    payout (e.g. log scale), and weight-vs-risk (why you don't play the
    lottery)

game theory
===========

- pareto efficiency aka pareto optimality: a possibly local optimum
  - can't improve one person without worsening another
- nash equilibrium: nobody can improve by changing only own strategy
  unilaterally
  - assumes each player knows equilibrium strategies of others
  - doesn't mean there aren't strategies that are better for all
    - eg competing businesses forming cartel to increase profits
  - may appear irrational to 3rd party, since not necessarily pareto optimal

puzzles

- pirates
- prisoner's dilemma
  - if A testifies against B and B is silent, A goes free and B gets full 10yr
  - if both silent, both get 6mo
  - if both testify, both get 5yr
  - regardless of what opponent chooses, you always are better off betraying
  - nash equilibrium not nec pareto-optimal
  - non-0-sum game
- hotelling's game
  - 2 competing hot dogs $(i,j)$ stand on beach that ranges over $[-1, 1]$
  - customers go to nearest stand
  - stands will naturally go to $(0,0)$, even though social opt is $(-.5,.5)$

number theory
=============

- fermat's last theorem: no 3 pos ints $a,b,c$ can satisfy $a^n+b^n=c^n$ for
  any int $n>2$
  - long-standing conjecture; proven in 1995

misc
====

- harmonic series: $\sum_{n=1}^{\infty}\frac{1}{n}=\infty$
- for density function $f(x)$ with one hump, choose $a,b$ on opposite sides of
  the hump with $f(a) = f(b)$, then $[a, b]$ is the shortest interval with its
  mass
  - useful in statistics and more
  - <http://www.johndcook.com/blog/2009/02/23/finding-shortest-confidence-interval/>

queueing theory
===============

- little's law: $L=\lambda W$ where $L$ is avg number of customers in system,
  $\lambda$ is avg arrival rate, $W$ is avg time each customer spends in system

sequences and series
====================

- TODO name: $(1-1/x)^k \approx e^{-k/x}$, $(1-k/m)^n \approx e^{-kn/m}$ (last
  seen <http://www.eecs.harvard.edu/~michaelm/postscripts/im2005b.pdf>)
