TODO merge in notes from CS170
TODO http://www.catonmat.net/blog/summary-of-mit-introduction-to-algorithms/
TODO http://cstheory.stackexchange.com/questions/189/algorithms-from-the-book

TODO

- basics
  - the chinese remainder theorem (really an algo)
  - the euclidean algorithm
  - gaussian elimination
  - linear programming
  - primality testing
  - fast matrix product
  - tsp
  - integer factoring
- combinatorial optimization
  - minimum spanning tree
  - shortest path in graphs
  - for-fulkerson max-flow algorithm
  - gale-shapley stable marriage
  - general matching in graphs
- randomized algos
  - primality testing
  - volume of convex bodies
  - counting matchings
  - min-cut of graphs
  - string matching
  - hashing
  - gibbs sampling
  - markov chain monte carlo (MCMC)
- heuristic algos
  - local search
  - shotgun sequencing algorithm
  - simulated annealing
- real-world impact
  - fast fourier transform
  - rsa encryption
  - miller-rabin primality test
  - reed-solomon codes
  - lempel-ziv compression
  - page rank of google
  - consistent hashing of akamai
  - viterbi and hidden markov models
  - smith-waterman
  - spectral low rank approximation algorithms
  - binary decision diagrams
- info theory
  - digital fountain codes
  - cdma
  - compressive sensing of images
- <http://rjlipton.wordpress.com/2009/10/27/highlights-of-focs-theory-day/>

misc terms

- _binary decision diagram (BDD)_: binary tree representation of truth table;
  each layer switches on one input

core concepts

- TODO: Range counting, predecessor, voronoi diagrams, dynamic optimality, planar point location, nearest neighbor, partial sums, connectivity
- skim adv algos
- <http://mybiasedcoin.blogspot.com/2009/10/core-tcs.html>

hash tables

- direct chaining: list per bucket; optional move-to-front heuristic
- linear scanning aka open addressing: try other slots
  - try $h$, $h+1$, $h+2$, ...
  - try $h_1$, $h_2$, $h_3$, ...
- cuckoo hashing: $k$ (usu. 2) hash fn's
  - insert: into one of the 2 locs, displacing any resident and repeat the
    insertion of the resident, possibly evicting multiple keys
  - lookup: inspect 2 locs
  - inserts succeed in expected const time, even amortizing table rebuilds
    (growths), as long as load factor <50%; with 3 fn's, up to <91% load
  - also considering $n$-way tables; with 2 keys per bucket, up to <80% load
  - much faster than chained hashing for small, cache-resident hash tables on
    modern processors
  - $n$-way buckets faster than conventional methods for large tables

hash functions

- _rotating hashes_: for open addressing, avoid computing a bunch of different
  hash functions; just compute one long hash value and apply sliding window on
  it to get a bunch of shorter hash values

bloom filter

- TODO
- TODO scalable bloom filters
- $m = -n*ln(p)/(ln(2)^2)$, where $m$ is # bits given $n$ elts, desired false prob $p$
- $k = 0.7*m/n$, where $k$ is # hash fns
- sometimes bits are shared by all hash fns; sometimes partition up into
  disjoint ranges for each fn; same asymptotic behavior
- [Network Applications of Bloom Filters: A Survey](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.9672&rep=rep1&type=pdf)

prime sieve

  given n, return all primes up through n
  isprime = [true] * n
  for i in [2..n]
    for j in [2i, 3i, ..., n]
      isprime[j] = false
  return [ i for i in [2..n] where isprime[i] ]

gcd

  more concise (yay mod operator):
  gcd(a,b) =
    if b = 0, a
    else,     gcd(a, b % a)

  explicit:
  gcd(a,b) =
    if b = 0, a
    if a < b, gcd(b, a)      // swap
    else,     gcd(a, b % a)

self-balancing binary search trees

- red-black
  - leaf nodes don't contain data; sometimes use single sentinal
  - root, leaf nodes are black
  - red nodes have black children
  - every simple path from any node to any descendant leaf contains same number
    of black nodes
- AVL
- splay

string search

- knuth morris pratt
- boyer-moore
- boyer-moore-horspool

hashing

- locality sensitive hashing
- perfect hashing
- universal hashing
- linear probing, linear hashing

spatial indexing

- quadtree: 4-ary tree (child per quadrant)
- geohashing: treat each quadtree as trie, and address the quadrants as 00, 01,
  10, and 11; then leaves are `00 11 01 10 00`
  - to query region intersection, can just choose leaves for tightest bound,
    but that's slow/numerous
  - instead, specify max number of regions, then recursively choose the
    quadrant that when subdivided will remove the most area while staying under
    max
  - visit selected areas in "Z" order; this limits the continuity
- hilbert curves: "U" order; better locality behavior [for convex hulls?]
  - a type of 1D fractal known as a _space-filling curve_
  - can translate (x,y) to hilbert curve pos
- <http://blog.notdot.net/2009/11/Damn-Cool-Algorithms-Spatial-indexing-with-Quadtrees-and-Hilbert-Curves>

bin packing

- NP-hard
- items of diff sizes must be packed into identical bins; minimize # bins
- formally: given bin size $V$ and item sizes $a_1,\dots,a_n$, find int $B$ and
  $B$-partition $S_1 \cup \dots \cups S_B$ of ${1,\dots,n}$ such that $\sum_{i
  \in S_k} a_i \le V$ for all $k \in 1,\dots,B$

integer linear programming aka integer programming

- NP-hard
- maximize $\mathbf{c}^T \mathbf{x}$ subject to $A \mathbf{x} \le \mathbf{b}$

linear programming

- simplex algorithm: simplest; usu efficient but poor worst-case behavior
- both simplex-based methods and interior point methods have similar efficiency
  for routine linear programs
- in P with ellipsoid algorithm or interior point methods

randomized
==========

- reservoir sampling: sample $n$ elts from stream of unknown len in 1 pass
  - keep buffer (_reservoir_) of $n$ elts, where elts are $x_i$
  - $\forall x_i, i>n$, with prob $n/i$ replace random buffered elt with $x_i$
  - prob $n/i$ is prob of no combination of $n$ elts containing $x_i$ (or any
    particular elt in a set of $i$ elts)
- weighted reservoir sampling: each elt has weight
  - maintain total seen weight and total reservoir weight

sort
====

TODO heap, insertion, selection, bubble, radix, quick

in-place quicksort (c.a.r. hoare)

- select a pivot, possibly doing so using sampling
- rather than building lists less/equal/greater, walk the list from both ends
  and swap as necessary (to construct sublists in-place)

timsort

- adaptive, stable, natural mergesort; hybrid with insertion sort
- supernatural perf on many kinds of partially ordered/reversed arrays
  - less than lg(N!) cmp's needed; few as N-1
  - as fast as Python's previous highly tuned samplesort hybrid on random arrays
- pseudocode: TODO
- use
  - python's default list.sort
  - java 7's new Arrays.sort
- <http://svn.python.org/projects/python/trunk/Objects/listsort.txt>
- <http://en.wikipedia.org/wiki/Timsort>
- <http://stackoverflow.com/questions/1733073/grokking-timsort>

dynamic programming
===================

dynamic time-warping

edit distance

- levenshtein distance: the standard way of measuring distance; ins/del/upd
  - damerau-levenshtein: also transposition of two chars
- hamming distance: considers only substitutions in same-length strings

subsequence sum

- given $x_i$, $A_i$ is max subsequence sum up to and including $x_i$

  $$A_i = \max{ A_{i-1}, 0 } + x_i$$

compression
===========

- huffman coding
  - variable length coding: more freq symbols given shorter codes
- LZ77
  - sliding window compression, replacing strings with backrefs
    ((length,offset) pairs)
  - strings must be within window size apart
- LZ78: like LZ77 but looking forward as well; less popular
- lempel-ziv-welch (LZW): patent-encumbered modification of LZ78
- deflate
  - intended to be patent-unencumbered replacement for LZW
  - LZ77 then huffman coding, but uses one huffman tree for char literals and
    length codes, and another for backwards offsets
- zip
  - permits multiple compression algos, but deflate dominant
  - can take collections of files, but compresses each individually, so can't
    exploit redundancy across files
- LZMA: lempel-ziv markov chain
  - used in 7z
- burrows-wheeler transform (BWT)
  - sort all rotations of a string (padded with start and end) and take last
    col
  - reversible: last col gives all chars; sort to get first col; last and first
    give all pairs of successive chars
- bzip2: BWT + MTF transform + Huffman coding; parallelizable; slower than gzip
  - move-to-front (MTF) transform: decrease entropy; usu useful only after BWT
- prediction by partial matching (PPM): smaller/slower than bzip2
- lempel-ziv-oberhumer (LZO): very fast decompression
  - block-based (parallelizable)
  - similar compression speed as deflate
  - added to hadoop by twitter
- zippy: google's fast compression algo
  - zippy: 300MB/s encode, 600MB/s decode, 2-4X compression
  - gzip: 25MB/s encode, 200MB/s decode, 4-6X compression
  - <http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf>
- implementations:
  - gzip: uses zlib
  - zlib: deflate; compresses single files (freq. tar); not parallelizable

[On Reducing the Size of Compressed Javascript (by up to 20%)](http://timepedia.blogspot.com/2009/08/on-reducing-size-of-compressed.html)

- browser only supports gzip; implementing lzma/ppm in js too slow
- gzip = LZ77 + huffman; huffman uses separate huffman tree for backwards
  offset
  - arrange for the backwards offsets to be the same
- sort functions to cluster by similarity, eg tf-idf, edit distance
  - impl: use edit distance, then recursively combine best-matching pairs
- other tricks:
  - larger-base charset for var renames
  - rename from bottom-up scopes to reuse vars
  - stable ordering of renames (so always get f(a,b,c))

high-perf linear algebra
========================

TODO singular value decomposition (SVD)

- many apps in signal processing and statistics
- important factorization of rectangular real or complex matrix

TODO fast-fourier transform (FFT)

- compute (inverse) discrete FT
- FFT on columns
- multiply by twiddle factors
- FFT on rows
- transposes
- butterfly diagram: used to describe FFT; portion of compute that combines
  smaller DFTs into larger DFT

TODO parallel HPC algorithms

coding/information theory
=========================

error detection coding

- repetition codes: just repeat the original signal
- checksum: modular sum of words/bytes in msg
- parity bit: simplest 1-bit checksum
  - even: 1 iff # ones is even; ditto for odd
  - special case of CRC, where 1-bit CRC is generated by polynomial $x+1$
- cyclic redundancy check: non-secure hash fn based on _cyclic codes_
  - characterized by generator polynomial
  - there's a table of commonly used/standard CRCs
- crypto hash function: infeasible to find input with same hash
- eg: Ethernet has CRC-32, IPv4 has 2B checksum over header, IPv6 has none, UDP
  has checksum over UDP/IP headers (optional over IPv4), TCP has checksum over
  TCP/IP headers with ARQ via triple-ack or timeout

checksums

- internet checksum: 2 bytes; used in IPv4, TCP, UDP headers; very weak (amazon outage)
- adler-32: 4 bytes; stronger; still efficient to compute; weak for short msgs
- fletcher's checksum: slightly stronger than adler-32; slightly slower to compute
- CRC: good middle ground; studied by mathematicians; easily implemented in HW
  - CRC32C: particularly strong for communicating apps; implemented in SSE4.2
    - slicing-by-8: best SW algo for CRC32C
- <http://evanjones.ca/crc32c.html>

error correcting code (ECC): usu. mean FEC

- automatic repeat request (ARQ) aka backward error correction: use error
  detection codes so receiver can request re-xmits; more of a protocol
- hybrid ARQ (HARQ): combine FEC for minor errors & ARQ for major errors
- forward error correction (FEC): always include ECC
  - convolutional codes: bit-by-bit; esp. suitable for HW
    - Viterbi decoder: optimal decoding
  - block codes: block-by-block
    - early, inefficient
      - repetition codes
      - Hamming codes: single bit errors
      - multi-dimensional parity-check (MDPC) codes: put msg in grid & calc
        parity for each row & col
    - efficient
      - Reed-Solomon codes: popular; corrects $d$ errors with $2d$ check
        symbols
      - turbo codes: near-optimal
      - low-density parity-check (LDPC) codes: near-optimal
      - BCH codes: handles random and burst errors

misc
====

feed-forward bloom filters (dga)

- high-level idea: for 2-phase exact pattern matching on some target, first
  filter the target with bloom filter built from the patterns, then
  traditional-grep smaller datasets
- assumes that the matches are all of the same len (else what are the target
  extracts that you should be hashing against the bloom filter?)
  - take the min len; if too small then handle those patterns separately
- feed-forward bloom filters are regular bloom filters but where each positive
  test flips on bits in a second "shadow" array (initially zeroed), which can
  then be used to filter out irrelevant *patterns*
- paper analysis shows high prob that there will be only few patterns in second
  array that don't get any matches
